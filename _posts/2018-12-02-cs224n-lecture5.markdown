---
layout: "post"
title: "cs224n lecture 5 note: explanation for backpropagation"
categories: [cs224n]
tags: [notes]
date: "2018-12-02 18:43"
---

终于忙完开题一切事宜，继续学习斯坦福公开课。

# 总结

这节课从4个不同层面讲反向传播，我并不打算把这几种都这样记录下来，因为归根结底，都是在体现链式规则的思想。

最令我感到清晰明了的就是电路解释那一块儿，它在传达给学习者一种直觉，不管神经网络有多少层，在求当前层偏导的时候只需要用到局部对应参数的梯度和上一层传来的信号。

如下图所示：

![manners]({{ site.url }}/assets/images/cs224n/lecture5/manners.jpg)

网络图和运算过程如下图所示：

![network]({{ site.url }}/assets/images/cs224n/lecture5/network.jpg)

当我在自己推导的时候，神经网络反方向按从低到高来看，令我感到棘手的恰恰是高层梯度的求解。而利用上面的思想就显得清晰起来。视频上求 $\frac{\partial S}{\partial W^{(1)}}$ 在用到上层信号的时候直接用的是 $W^{(2)T}\delta^{(3)}$ ，我一时没反应过来，印象中这是上节课对x求偏导的表达式，可是跟W有什么关系？其实，不管是基于链式法则还是电路解释，对W求微分，都要用到对上层结点的偏导值，而这本质上就是像第四节最后那样，求x偏导（其实就是在求对结点的偏导）。理通顺之后，就好说了，可以得到这样的递推公式。

![recursive]({{ site.url }}/assets/images/cs224n/lecture5/recursive.jpg)

bp过程我们都明白，我们以为自己能够写出来，大量的深度学习框架让我们停止了思考，然而，正如视频老师所说的那样，有时候是需要自己去写的，我想我们还是能写出来，但肯定是看起来很繁琐的，这两节课真正教给我的，不是直接的理论，而是技巧，让自己容易理解的技巧和让电脑快速计算的技巧。

* 数据复用：$\delta$的引用真是简化了运算，有效地利用了重复数据
* 化繁为简：对向量求偏导可以先写对向量某一元素求偏导的计算过程，既好理解，又容易求
* 化散为整：将有求和符号的或单一元素之间运算的转化为矩阵相乘，简洁，且在计算机中已被优化，计算迅速

上次博客也说了对上节课有些地方没整明白，这次又重复观看了第四课，再加上自己手写推导，都搞明白了。最后附上自己的推导过程。

![write]({{ site.url }}/assets/images/cs224n/lecture5/write.jpg)
